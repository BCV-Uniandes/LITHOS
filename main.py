import os
import time
import argparse
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.autograd import Variable
import torch.backends.cudnn as cudnn
from torch.utils.data import DataLoader, WeightedRandomSampler
from torchvision import transforms
from torchvision.transforms import v2
import utils
from data_loader import LithosDataset
from tqdm import tqdm
import meters.logging as logging
import meters.misc as misc
from fvcore.common.timer import Timer
from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR
from sklearn.metrics import precision_recall_curve, precision_score, recall_score, f1_score, average_precision_score
import matplotlib.pyplot as plt
import wandb
from torchvision.models import vit_l_16, ViT_L_16_Weights
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, precision_recall_curve, auc
)

# Logging
logger = logging.get_logger(__name__)  

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--lr', type=float, default=1e-3, help='Initial learning rate (default: 1e-3)')
    parser.add_argument('--epochs', type=int, default=10, help='Maximum number of epochs (default: 10)')
    parser.add_argument('--batch', type=int, default=64, help='Batch size (default: 64)')
    parser.add_argument('--fold', type=int, default=1, help='Fold to train (default: 1)')
    parser.add_argument('--test', action='store_true', default=False, help='Only test the model')
    parser.add_argument('--ft', action='store_true', default=False, help='Fine-tune a model')
    parser.add_argument('--resume', action='store_true', default=False, help='resume training a model')
    parser.add_argument('--model_type', type=str, default='PolarVit', help='Model Architecture (default: PolarVit)')
    parser.add_argument('--name', type=str, default='PolarVit_exp', help='Name of the experiment (default: PolarVit_exp)')
    parser.add_argument('--load_model', type=str, default='last', help='Weights to load (default: best_acc)')
    parser.add_argument('--n_classes', type=int, default=2, help='Number of classes (default: 2)')
    parser.add_argument("--csv_files", type=str, default="/media/SSD7/LITHOS/COCO_ANNOTATIONS_INTERSECTION", help="Path to the CSV files of both folds") # TODO: The csv are now downloaded in dataset
    parser.add_argument("--root_dataset", type=str, default="/media/SSD7/LITHOS/SEMANTIC_SEGMENTATION_INTERSECTION_256x256", help="Root to the dataset") # TODO: This is also downloaded in dataset
    parser.add_argument("--balance_dataloader", action="store_true", default=False, help="Balance the dataloader")
    parser.add_argument("--balance_loss", action="store_true", default=False, help="Balance loss")
    parser.add_argument("--use_alpha", action="store_true", default=False, help="Use alpha parameter")
    parser.add_argument("--use_linear", action="store_true", default=False, help="Use linear transformation")
    parser.add_argument("--use_xpl", action="store_true", default=False, help="Use XPL data")
    args = parser.parse_args()

    global device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # Wandb initialization
    wandb.init(project="lithos-classification", entity="lithos", config=args.__dict__)
    run_id = wandb.run.id  # Unique ID generated by wandb
    logger.info(f"Starting run with id: {run_id}")
    wandb.config.update({"run_id": run_id})
    config = wandb.config
    training = not args.test
    global save_path
    save_path = os.path.join('experiments', args.name, 'fold' + str(args.fold), run_id)
    
    assert args.fold in [1, 2], "Fold should be either 1 or 2."

    train_fold = args.fold
    val_fold = 2 if args.fold == 1 else 1
    train_df = pd.read_csv(os.path.join(args.csv_files, f"Fold{train_fold}_complete_info_allclasses.csv"))
    val_df = pd.read_csv(os.path.join(args.csv_files, f"Fold{val_fold}_complete_info_allclasses.csv"))

    os.makedirs(save_path, exist_ok=True)

    # Seeds for reproducibility
    np.random.seed(0)
    torch.manual_seed(0)
    torch.cuda.manual_seed_all(0)
    cudnn.deterministic = False
    torch.backends.cudnn.benchmark = True

    # Logging setup
    logging.setup_logging(save_path)
    
    # Model setup
    if args.model_type == 'ViT':
        print("Model -> ViT")
        weights = ViT_L_16_Weights.IMAGENET1K_SWAG_LINEAR_V1
        preprocess = weights.transforms()
        model = vit_l_16(weights=weights)
        model.heads.head = nn.Linear(model.heads.head.in_features, args.n_classes)

    elif args.model_type == 'GoogLeNet':
        print("Model -> GoogLeNet")
        from torchvision.models import googlenet, GoogLeNet_Weights
        weights = GoogLeNet_Weights.IMAGENET1K_V1
        preprocess = weights.transforms()
        model = googlenet(weights=weights)
        model.fc = nn.Linear(model.fc.in_features, args.n_classes)

    elif args.model_type == 'ResNet':
        print("Model -> ResNet")
        from torchvision.models import resnet50, ResNet50_Weights
        weights = ResNet50_Weights.IMAGENET1K_V1
        preprocess = weights.transforms()
        model = resnet50(weights=weights)
        model.fc = nn.Linear(model.fc.in_features, args.n_classes) # Change the head to the number of classes

    elif args.model_type == 'PolarViT':
        print("Model -> PolarViT")
        from polar_vit_arch import PolarViT, Decoder, Config
        weights = ViT_L_16_Weights.IMAGENET1K_SWAG_LINEAR_V1
        preprocess = weights.transforms()
        # Load first ViT model
        vit1 = vit_l_16(weights=weights) 
        vit1.heads.head = nn.Linear(vit1.heads.head.in_features, args.n_classes)
        # Load second ViT model
        vit2 = vit_l_16(weights=weights)
        vit2.heads.head = nn.Linear(vit2.heads.head.in_features, args.n_classes)

        config_decoder = Config()
        decoder_1 = Decoder(config_decoder.hidden_size, config_decoder.num_layers, config_decoder.num_attention_heads)
        decoder_2 = Decoder(config_decoder.hidden_size, config_decoder.num_layers, config_decoder.num_attention_heads)
        classification_head = vit2.heads.head
        
        model = PolarViT(vit1, vit2, decoder_1, decoder_2, classification_head, use_alpha=args.use_alpha, use_linear=args.use_linear)
    
    model = model.to(device)
    else:
        print("Model not implemented yet")
        return
    
    # Optimizer and scheduler setup
    if config.optimizer == "Adam":
        optimizer = optim.Adam(model.parameters(), lr=config.lr)
    elif config.optimizer == "SGD":
        optimizer = optim.SGD(model.parameters(), lr=config.lr, momentum=config.momentum)

    # Define scheduler according to wandb config
    if config.scheduler == "StepLR":
        scheduler = StepLR(optimizer, step_size=config.step_size, gamma=config.gamma)
    elif config.scheduler == "CosineAnnealingLR":
        scheduler = CosineAnnealingLR(optimizer, T_max=config.T_max)

    args.epoch = 0
    best_acc = 0
    best_f1 = 0

    # Load a model if resume or testing 
    if args.resume or not training:
        name = 'epoch_' + args.load_model + '.pth.tar'
        checkpoint = torch.load( os.path.join(save_path, name), map_location=lambda storage, loc: storage)
        args.epoch = checkpoint['epoch']
        best_acc = checkpoint['best_acc']
        best_f1 = checkpoint['best_f1']
        args.lr = checkpoint['lr']
        print('Loading model and optimizer {}.'.format(args.epoch))
        model.load_state_dict(checkpoint['state_dict'], strict=not args.ft)
        optimizer.load_state_dict(checkpoint['optimizer'])

    # Data augmentation for training
    data_transform = v2.Compose([
        v2.ToPILImage(),
        v2.RandomHorizontalFlip(),
        v2.RandomVerticalFlip(),
        v2.RandomRotation(degrees=45),
        v2.ToTensor(),
        ])
    
    train_ds = LithosDataset(
            df=train_df,
            polar_vit=args.model_type == 'PolarViT',
            root_dataset=args.root_dataset,
            preprocess=preprocess, 
            transform=data_transform,
            use_xpl=args.use_xpl,
            )
    val_ds = LithosDataset(
            df=val_df,
            polar_vit=args.model_type == 'PolarViT',
            root_dataset=args.root_dataset,
            preprocess=preprocess,
            use_xpl=args.use_xpl,
            )

    print('---> Train Images: {}'.format(train_df.__len__()))
    print('---> Val Images: {}'.format(val_df.__len__()))

    # Data loader
    if args.balance_loss or args.balance_dataloader:
        class_weights = torch.FloatTensor([1.0 / freq for freq in train_ds._get_weights_class()]).to(device)
    sampler =  None
    if args.balance_dataloader:
        sample_weights = [class_weights[rta["target"]] for rta in tqdm(train_ds)]
        sampler = WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)
        train_loader = DataLoader(train_ds, batch_size=args.batch, sampler=sampler, num_workers=2)
    else:
        train_loader = DataLoader(train_ds, shuffle=True, batch_size=args.batch, sampler=sampler, num_workers=2)
    
    # Validation loader
    val_loader = DataLoader(val_ds, shuffle=False, batch_size=args.batch, num_workers=2)

    if args.balance_loss:
        loss = nn.CrossEntropyLoss(weight=torch.FloatTensor(class_weights)).to(device)
    else:
        loss = nn.CrossEntropyLoss().to(device)
    
    if training:
        out_file = open(os.path.join(save_path, 'progress.csv'), 'a+')
        # Logging
        logger.info('Training in ----> Fold {}'.format(str(args.fold)))
        logger.info('Training with {} samples'.format(len(train_loader.dataset)))
        logger.info('Val in ----> Fold {}'.format("1" if args.fold==1 else "2"))
        logger.info('Val with {} samples'.format(len(val_loader.dataset)))
        logger.info("Start epoch: {}. Final epoch: {}".format(args.epoch + 1, args.epochs))        
        # Time
        iter_timer = Timer()

        for epoch in range(args.epoch + 1, args.epochs + 1):
            args.epoch = epoch
            lr = utils.get_lr(optimizer)
            wandb.log({"lr": lr})
            logger.info('--------- Starting Epoch {}/{} --> {} ---------'.format(epoch, args.epochs, time.strftime("%H:%M:%S")))
            logger.info('Current learning rate: {}'.format(lr))
            train_loss = train(args, model, train_loader, optimizer, loss)
            acc = 0
            val_loss = -1
            mAP =  -1
            F1 =  -1
            val_loss, acc, precision, recall, F1, roc_auc, pr_auc = val(args, model, val_loader, loss, epoch)

            out_file.write('{},{},{},{},{},{},{},{},{}, {}\n'.format(
                args.epoch,
                train_loss,
                val_loss,
                lr,
                acc,
                roc_auc,
                pr_auc,
                F1,
                precision,
                recall
            ))
            out_file.flush()
            scheduler.step()
            
            # Update best metrics
            is_best_current_f1 = F1 > best_f1
            is_best_current_acc = acc > best_acc
            best_acc = max(best_acc, acc)
            best_f1 = max(best_f1, F1)

            if not np.isnan(train_loss.item()):
                state = {
                    'epoch': epoch,
                    'state_dict': model.state_dict(),
                    'optimizer': optimizer.state_dict(),
                    'loss': [train_loss, val_loss],
                    'lr': lr,
                    'acc': acc,
                    'best_acc': best_acc, 
                    'best_f1': best_f1}
                utils.save_epoch(state, save_path, epoch, checkpoint=False, is_best_acc=is_best_current_acc, is_best_f1=is_best_current_f1)
            wandb.log({"epoch": epoch})
        out_file.close()
        iter_timer.pause()
        logger.info('Training completed in {} minutes'.format(iter_timer.seconds()//60))
    
    elif args.test:
        model.eval()
        val_loss, acc, precision, recall, F1, roc_auc, pr_auc = val(args, model, val_loader, loss)
        logger.info('Test with {} samples'.format(len(val_loader.dataset)))
        logger.info('Test loss: {:.6f} \tAccuracy: {:.3f} Precision: {:.3f} Recall {:.3f} F1 {:.3f} ROC_AUC {:.3f} PR_AUC {:.3f}'.format(
        val_loss, 
        acc, 
        precision, 
        recall,
        F1,
        roc_auc,
        pr_auc))

def train(args, model, loader, optimizer, loss_fn):
    wandb.watch(model, criterion=loss_fn, log="all", log_freq=10)
    model.train()
    epoch_loss = utils.AverageMeter()
    batch_loss = utils.AverageMeter()
    print_stats = 1
    for batch_idx, sample in enumerate(loader):
        if args.model_type == 'PolarVit':
            data = Variable(sample['data1'].float()).to(device)
            data2 = Variable(sample['data2'].float()).to(device)
            target = Variable(sample['target']).to(device)
            optimizer.zero_grad()
            out = model(data, data2)
        else:
            data = Variable(sample['data'].float()).to(device)
            target = Variable(sample['target']).to(device)
            optimizer.zero_grad()
            out = model(data)

        loss = loss_fn(out, target)
        loss.backward()
        optimizer.step()
        batch_loss.update(loss)
        epoch_loss.update(loss)
                
        if batch_loss.count % print_stats == 0:
            text = '{} -- [{}/{} ({:.0f}%)]\tLoss: {:.6f} -- gpu_mem: {:.2f} GB'
            logger.info(text.format(
                time.strftime("%H:%M:%S"), (batch_idx + 1),
                (len(loader)), 100. * (batch_idx + 1) / (len(loader)),
                batch_loss.avg, misc.gpu_mem_usage()))
            batch_loss.reset()
        wandb.log({"train_loss_batch": loss.item()})
    wandb.log({"train_loss_epoch": epoch_loss.avg})
    logger.info('--- Train: \tLoss: {:.6f} --- RAM: {:.2f}/{:.2f} GB  gpu_mem: {:.2f} GB'.format(epoch_loss.avg, \
                 *misc.cpu_mem_usage(), misc.gpu_mem_usage()))
    return epoch_loss.avg

def val(args, model, loader, loss_fn, epoch=0):
    print("-------------Validating-------------")
    model.eval()
    epoch_loss = utils.AverageMeter()
    
    all_outputs = []
    all_preds = []
    all_targets = []

    for sample in tqdm(loader):
        if args.model_type == 'PolarVit':
            data = Variable(sample['data1'].float()).to(device)
            data2 = Variable(sample['data2'].float()).to(device)
            target = Variable(sample['target']).to(device)
            with torch.no_grad():
                out = model(data, data2)
        else:
            data = Variable(sample['data'].float()).to(device)
            target = Variable(sample['target']).to(device)
            with torch.no_grad():
                out = model(data)

        loss = loss_fn(out, target)
        sm = nn.Softmax(dim=1)
        output_sm = sm(out)

        if args.n_classes == 2:
            all_outputs.extend(output_sm[:, 1].cpu().numpy())  # Probability of class 1
        else:
            all_outputs.extend(output_sm.cpu().numpy())  # Probabilities for all classes

        _, preds = torch.max(output_sm, 1)
        all_preds.extend(preds.cpu().numpy())
        all_targets.extend(target.cpu().numpy())
        epoch_loss.update(loss.item())
    
    all_outputs = np.array(all_outputs)
    all_targets = np.array(all_targets)
    all_preds = np.array(all_preds)

    if args.n_classes == 2:
        precision = precision_score(all_targets, all_preds)
        recall = recall_score(all_targets, all_preds)
        F1 = f1_score(all_targets, all_preds)
        accuracy = accuracy_score(all_targets, all_preds)
        roc_auc = roc_auc_score(all_targets, all_outputs)
        precision_vals, recall_vals, _ = precision_recall_curve(all_targets, all_outputs)
        pr_auc = auc(recall_vals, precision_vals)        
        
        wandb.log({
            "val_loss": epoch_loss.avg,
            "val_accuracy": accuracy,
            "val_precision": precision,
            "val_recall": recall,
            "val_f1_score": F1,
            "val_roc_auc": roc_auc,
            "val_pr_auc": pr_auc
        })

        print(f"Accuracy: {accuracy}, Loss: {epoch_loss.avg}, Precision: {precision}, Recall: {recall}, F1: {F1}, roc_auc: {roc_auc}, pr_auc: {pr_auc}")

        logger.info('--- Val: \tLoss: {:.6f}  \tAccuracy: {:.3f} F1: {:.3f} Precision: {:.3f} Recall: {:.3f} Roc AUC: {:.3f} PR AUC {:.3f} --- RAM: {:.2f}/{:.2f} GB  gpu_mem: {:.2f} GB '.format(
                epoch_loss.avg, accuracy, F1, precision, recall, roc_auc, pr_auc, *misc.cpu_mem_usage(), misc.gpu_mem_usage()))
    else:
        precision = precision_score(all_targets, all_preds, average='macro')
        recall = recall_score(all_targets, all_preds, average='macro')
        F1 = f1_score(all_targets, all_preds, average='macro')
        accuracy = accuracy_score(all_targets, all_preds)
        try:
            roc_auc = roc_auc_score(all_targets, all_outputs, multi_class='ovr')  # One-vs-Rest strategy
        except:
            roc_auc = 0

        # Precision-recall curve 
        precision_vals = dict()
        recall_vals = dict()
        pr_auc_vals = dict()

        for class_idx in range(args.n_classes):
            precision_vals[class_idx], recall_vals[class_idx], _ = precision_recall_curve(
                (all_targets == class_idx).astype(int), all_outputs[:, class_idx]
            )
            pr_auc_vals[class_idx] = auc(recall_vals[class_idx], precision_vals[class_idx])

        # Compute average PR-AUC
        pr_auc = np.mean(list(pr_auc_vals.values()))

        wandb.log({
            "val_loss": epoch_loss.avg,
            "val_accuracy": accuracy,
            "val_precision_macro": precision,
            "val_recall_macro": recall,
            "val_f1_score_macro": F1,
            "val_roc_auc": roc_auc,
            "val_pr_auc": pr_auc
        })
        
        print(f"Accuracy: {accuracy}, Loss: {epoch_loss.avg}, Precision (macro): {precision}, Recall (macro): {recall}, F1 (macro): {F1}, roc_auc: {roc_auc}, pr_auc: {pr_auc}")

        logger.info('--- Val: \tLoss: {:.6f}  \tAccuracy: {:.3f} F1: {:.3f} Precision: {:.3f} Recall: {:.3f} Roc AUC: {:.3f} PR AUC {:.3f} --- RAM: {:.2f}/{:.2f} GB  gpu_mem: {:.2f} GB '.format(
                epoch_loss.avg, accuracy, F1, precision, recall, roc_auc, pr_auc, *misc.cpu_mem_usage(), misc.gpu_mem_usage()))
    
    return epoch_loss.avg, accuracy, precision, recall, F1, roc_auc, pr_auc

if __name__ == '__main__':
    main()
