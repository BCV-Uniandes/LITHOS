import os
import time
import argparse
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.autograd import Variable
import torch.backends.cudnn as cudnn
from torch.utils.data import DataLoader, WeightedRandomSampler
from torchvision import transforms
from torchvision.transforms import v2
import utils
from data_loader import LithosDataset
from tqdm import tqdm
import meters.logging as logging
import meters.misc as misc
from fvcore.common.timer import Timer
from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR
from sklearn.metrics import precision_recall_curve, precision_score, recall_score, f1_score, average_precision_score
import matplotlib.pyplot as plt
import wandb
from torchvision.models import vit_l_16, ViT_L_16_Weights, googlenet, GoogLeNet_Weights, resnet50, ResNet50_Weights
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, precision_recall_curve, auc
)

# Logging
logger = logging.get_logger(__name__)  

def main():
    parser = argparse.ArgumentParser()
    group = parser.add_mutually_exclusive_group()
    group.add_argument(
        "--use_alpha",
        dest="fusion_mode",
        action="store_const",
        const="alpha",
        help="Use learnable Î±-gate fusion (default)"
    )
    group.add_argument(
        "--use_linear",
        dest="fusion_mode",
        action="store_const",
        const="linear",
        help="Use nn.Linear on concatenated features"
    )
    parser.set_defaults(fusion_mode="alpha")

    parser.add_argument('--lr', type=float, default=1e-3, help='Initial learning rate (default: 1e-3)')
    parser.add_argument('--epochs', type=int, default=10, help='Maximum number of epochs (default: 10)')
    parser.add_argument('--batch', type=int, default=64, help='Batch size (default: 64)')
    parser.add_argument('--fold', type=int, default=1, help='Fold to train (default: 1)')
    parser.add_argument('--test', action='store_true', default=False, help='Only test the model')
    parser.add_argument('--ft', action='store_true', default=False, help='Fine-tune a model')
    parser.add_argument('--resume', action='store_true', default=False, help='resume training a model')
    parser.add_argument('--model_type', type=str, default='PolarViT', help='Model Architecture (default: PolarViT)')
    parser.add_argument('--name', type=str, default='PolarVit_exp', help='Name of the experiment (default: PolarViT_exp)')
    parser.add_argument('--load_model', type=str, default='last', help='Weights to load (default: best_acc)')
    parser.add_argument('--n_classes', type=int, default=2, help='Number of classes (default: 2)')
    parser.add_argument("--root_dataset", type=str, default="LITHOS_DATASET_P/LITHOS_DATASET", help="Root to the dataset")
    parser.add_argument("--balance_dataloader", action="store_true", default=False, help="Balance the dataloader")
    parser.add_argument("--balance_loss", action="store_true", default=False, help="Balance loss")
    parser.add_argument("--use_xpl", action="store_true", default=False, help="Use XPL data")
    parser.add_argument('--optimizer', type=str, default='Adam', choices=['Adam', 'SGD'], help='Optimizer type (default: Adam)')
    parser.add_argument('--momentum', type=float, default=0.9, help='Momentum for SGD (default: 0.9)')
    parser.add_argument('--scheduler', type=str, default='StepLR', choices=['StepLR', 'CosineAnnealingLR', 'None'], help='LR scheduler type (default: StepLR)')
    parser.add_argument('--step_size', type=int, default=10, help='Step size for StepLR scheduler (default: 10)')
    parser.add_argument('--gamma', type=float, default=0.1, help='Gamma for StepLR scheduler (default: 0.1)')
    parser.add_argument('--T_max', type=int, default=50, help='T_max for CosineAnnealingLR scheduler (default: 50)')
    args = parser.parse_args()
    use_alpha  = (args.fusion_mode == "alpha")
    use_linear = (args.fusion_mode == "linear")

    global device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # Initialization
    wandb.init(project="lithos-classification", entity="lithos", config=args.__dict__)
    run_id = wandb.run.id  # Unique ID generated by wandb
    logger.info(f"Starting run with id: {run_id}")
    wandb.config.update({"run_id": run_id})
    config = wandb.config
    training = not args.test
    global save_path
    save_path = os.path.join('experiments', args.name, 'fold' + str(args.fold), run_id)
    assert args.fold in [1, 2], "Fold should be either 1 or 2."
    train_fold = args.fold
    val_fold = 2 if args.fold == 1 else 1
    train_df = pd.read_csv(os.path.join(f"Fold{train_fold}_complete_info_allclasses.csv"))
    val_df = pd.read_csv(os.path.join(f"Fold{val_fold}_complete_info_allclasses.csv"))

    os.makedirs(save_path, exist_ok=True)

    # Seeds for reproducibility
    np.random.seed(0)
    torch.manual_seed(0)
    torch.cuda.manual_seed_all(0)
    cudnn.deterministic = False
    torch.backends.cudnn.benchmark = True

    # Logging setup
    logging.setup_logging(save_path)
    
    # Model setup
    if args.model_type == 'ViT':
        print("Model -> ViT")
        weights = ViT_L_16_Weights.IMAGENET1K_SWAG_LINEAR_V1
        preprocess = weights.transforms()
        model = vit_l_16(weights=weights)
        model.heads.head = nn.Linear(model.heads.head.in_features, args.n_classes)

    elif args.model_type == 'GoogLeNet':
        print("Model -> GoogLeNet")
        weights = GoogLeNet_Weights.IMAGENET1K_V1
        preprocess = weights.transforms()
        model = googlenet(weights=weights)
        model.fc = nn.Linear(model.fc.in_features, args.n_classes)

    elif args.model_type == 'ResNet':
        print("Model -> ResNet")
        weights = ResNet50_Weights.IMAGENET1K_V1
        preprocess = weights.transforms()
        model = resnet50(weights=weights)
        model.fc = nn.Linear(model.fc.in_features, args.n_classes)

    elif args.model_type == 'PolarViT':
        print("Model -> PolarViT")
        from polar_vit_arch import PolarViT, Decoder, Config
        weights = ViT_L_16_Weights.IMAGENET1K_SWAG_LINEAR_V1
        preprocess = weights.transforms()
        # Load first ViT model
        vit1 = vit_l_16(weights=weights) 
        vit1.heads.head = nn.Linear(vit1.heads.head.in_features, args.n_classes)
        # Load second ViT model
        vit2 = vit_l_16(weights=weights)
        vit2.heads.head = nn.Linear(vit2.heads.head.in_features, args.n_classes)

        config_decoder = Config()
        decoder_1 = Decoder(config_decoder.hidden_size, config_decoder.num_layers, config_decoder.num_attention_heads)
        decoder_2 = Decoder(config_decoder.hidden_size, config_decoder.num_layers, config_decoder.num_attention_heads)
        classification_head = vit2.heads.head
        
        model = PolarViT(vit1, vit2, decoder_1, decoder_2, classification_head, use_alpha=use_alpha, use_linear=use_linear)
    
    else:
        print("Model not implemented yet")
        return
    
    model = model.to(device)
    
    # Optimizer and scheduler setup
    if args.optimizer == "Adam":
        optimizer = optim.Adam(model.parameters(), lr=args.lr)
    elif args.optimizer == "SGD":
        optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)
    if args.scheduler == "StepLR":
        scheduler = StepLR(optimizer, step_size=args.step_size, gamma=args.gamma)
    elif args.scheduler == "CosineAnnealingLR":
        scheduler = CosineAnnealingLR(optimizer, T_max=args.T_max)
    else:
        scheduler = None

    args.epoch = 0
    best_acc = 0
    best_f1 = 0

    # Load a model if resume or testing 
    if args.resume or not training:
        name = 'epoch_' + args.load_model + '.pth.tar'
        checkpoint = torch.load( os.path.join(save_path, name), map_location=lambda storage, loc: storage)
        args.epoch = checkpoint['epoch']
        best_acc = checkpoint['best_acc']
        best_f1 = checkpoint['best_f1']
        args.lr = checkpoint['lr']
        print('Loading model and optimizer {}.'.format(args.epoch))
        model.load_state_dict(checkpoint['state_dict'], strict=not args.ft)
        optimizer.load_state_dict(checkpoint['optimizer'])

    # Data augmentation for training
    data_transform = v2.Compose([
        v2.ToPILImage(),
        v2.RandomHorizontalFlip(),
        v2.RandomVerticalFlip(),
        v2.RandomRotation(degrees=45),
        v2.ToTensor(),
        ])
    
    train_ds = LithosDataset(
            df=train_df,
            polar_vit=args.model_type == 'PolarViT',
            root_dataset=args.root_dataset,
            preprocess=preprocess, 
            transform=data_transform,
            use_xpl=args.use_xpl,
            num_classes = args.n_classes
            )

    val_ds = LithosDataset(
            df=val_df,
            polar_vit=args.model_type == 'PolarViT',
            root_dataset=args.root_dataset,
            preprocess=preprocess,
            use_xpl=args.use_xpl,
            num_classes = args.n_classes
            )

    print('---> Train Images: {}'.format(train_df.__len__()))
    print('---> Val Images: {}'.format(val_df.__len__()))

    # Data loader
    if args.balance_loss or args.balance_dataloader:
        class_weights = torch.FloatTensor([1.0 / freq for freq in train_ds._get_weights_class()]).to(device)
    sampler =  None
    if args.balance_dataloader:
        sample_weights = [class_weights[rta["target"]] for rta in tqdm(train_ds)]
        sampler = WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)
        train_loader = DataLoader(train_ds, batch_size=args.batch, sampler=sampler, num_workers=2)
    else:
        train_loader = DataLoader(train_ds, shuffle=True, batch_size=args.batch, sampler=sampler, num_workers=2)
    
    # Validation loader
    val_loader = DataLoader(val_ds, shuffle=False, batch_size=args.batch, num_workers=2)

    if args.balance_loss:
        loss = nn.CrossEntropyLoss(weight=torch.FloatTensor(class_weights)).to(device)
    else:
        loss = nn.CrossEntropyLoss().to(device)
    
    if training:
        out_file = open(os.path.join(save_path, 'progress.csv'), 'a+')
        # Logging
        logger.info('Training in ----> Fold {}'.format(str(args.fold)))
        logger.info('Training with {} samples'.format(len(train_loader.dataset)))
        logger.info('Val in ----> Fold {}'.format("1" if args.fold==1 else "2"))
        logger.info('Val with {} samples'.format(len(val_loader.dataset)))
        logger.info("Start epoch: {}. Final epoch: {}".format(args.epoch + 1, args.epochs))        
        # Time
        iter_timer = Timer()

        for epoch in range(args.epoch + 1, args.epochs + 1):
            args.epoch = epoch
            lr = utils.get_lr(optimizer)
            wandb.log({"lr": lr})
            logger.info('--------- Starting Epoch {}/{} --> {} ---------'.format(epoch, args.epochs, time.strftime("%H:%M:%S")))
            logger.info('Current learning rate: {}'.format(lr))
            train_loss = train(args, model, train_loader, optimizer, loss)
            acc = 0
            val_loss = -1
            mAP =  -1
            F1 =  -1
            val_loss, acc, precision, recall, F1, roc_auc, pr_auc = val(args, model, val_loader, loss, epoch)

            out_file.write('{},{},{},{},{},{},{},{},{}, {}\n'.format(
                args.epoch,
                train_loss,
                val_loss,
                lr,
                acc,
                roc_auc,
                pr_auc,
                F1,
                precision,
                recall
            ))
            out_file.flush()
            scheduler.step()
            
            # Update best metrics
            is_best_current_f1 = F1 > best_f1
            is_best_current_acc = acc > best_acc
            best_acc = max(best_acc, acc)
            best_f1 = max(best_f1, F1)

            if not np.isnan(train_loss.item()):
                state = {
                    'epoch': epoch,
                    'state_dict': model.state_dict(),
                    'optimizer': optimizer.state_dict(),
                    'loss': [train_loss, val_loss],
                    'lr': lr,
                    'acc': acc,
                    'best_acc': best_acc, 
                    'best_f1': best_f1}
                utils.save_epoch(state, save_path, epoch, checkpoint=False, is_best_acc=is_best_current_acc, is_best_f1=is_best_current_f1)
            wandb.log({"epoch": epoch})
        out_file.close()
        iter_timer.pause()
        logger.info('Training completed in {} minutes'.format(iter_timer.seconds()//60))
    
    elif args.test:
        model.eval()
        val_loss, acc, precision, recall, F1, roc_auc, pr_auc = val(args, model, val_loader, loss)
        logger.info('Test with {} samples'.format(len(val_loader.dataset)))
        logger.info('Test loss: {:.6f} \tAccuracy: {:.3f} Precision: {:.3f} Recall {:.3f} F1 {:.3f} ROC_AUC {:.3f} PR_AUC {:.3f}'.format(
        val_loss, 
        acc, 
        precision, 
        recall,
        F1,
        roc_auc,
        pr_auc))

def train(args, model, loader, optimizer, loss_fn):
    wandb.watch(model, criterion=loss_fn, log="all", log_freq=10)
    model.train()
    epoch_loss = utils.AverageMeter()
    batch_loss = utils.AverageMeter()
    print_stats = 1
    for batch_idx, sample in enumerate(loader):
        if args.model_type == 'PolarViT':
            data = Variable(sample['data1'].float()).to(device)
            data2 = Variable(sample['data2'].float()).to(device)
            target = Variable(sample['target']).to(device)
            optimizer.zero_grad()
            out = model(data, data2)
        else:
            data = Variable(sample['data'].float()).to(device)
            target = Variable(sample['target']).to(device)
            optimizer.zero_grad()
            out = model(data)

        loss = loss_fn(out, target)
        loss.backward()
        optimizer.step()
        batch_loss.update(loss)
        epoch_loss.update(loss)
                
        if batch_loss.count % print_stats == 0:
            text = '{} -- [{}/{} ({:.0f}%)]\tLoss: {:.6f} -- gpu_mem: {:.2f} GB'
            logger.info(text.format(
                time.strftime("%H:%M:%S"), (batch_idx + 1),
                (len(loader)), 100. * (batch_idx + 1) / (len(loader)),
                batch_loss.avg, misc.gpu_mem_usage()))
            batch_loss.reset()
        wandb.log({"train_loss_batch": loss.item()})
    wandb.log({"train_loss_epoch": epoch_loss.avg})
    logger.info('--- Train: \tLoss: {:.6f} --- RAM: {:.2f}/{:.2f} GB  gpu_mem: {:.2f} GB'.format(epoch_loss.avg, \
                 *misc.cpu_mem_usage(), misc.gpu_mem_usage()))
    return epoch_loss.avg

def val(args, model, loader, loss_fn, epoch=0):
    print("-------------Validating-------------")
    model.eval()
    epoch_loss = utils.AverageMeter()
    
    all_outputs = []
    all_preds = []
    all_targets = []

    for sample in tqdm(loader):
        if args.model_type == 'PolarViT':
            data = Variable(sample['data1'].float()).to(device)
            data2 = Variable(sample['data2'].float()).to(device)
            target = Variable(sample['target']).to(device)
            with torch.no_grad():
                out = model(data, data2)
        else:
            data = Variable(sample['data'].float()).to(device)
            target = Variable(sample['target']).to(device)
            with torch.no_grad():
                out = model(data)

        loss = loss_fn(out, target)
        sm = nn.Softmax(dim=1)
        output_sm = sm(out)

        if args.n_classes == 2:
            all_outputs.extend(output_sm[:, 1].cpu().numpy())  # Probability of class 1
        else:
            all_outputs.extend(output_sm.cpu().numpy())  # Probabilities for all classes

        _, preds = torch.max(output_sm, 1)
        all_preds.extend(preds.cpu().numpy())
        all_targets.extend(target.cpu().numpy())
        epoch_loss.update(loss.item())
    
    all_outputs = np.array(all_outputs)
    all_targets = np.array(all_targets)
    all_preds = np.array(all_preds)

    if args.n_classes == 2:
        precision = precision_score(all_targets, all_preds)
        recall = recall_score(all_targets, all_preds)
        F1 = f1_score(all_targets, all_preds)
        accuracy = accuracy_score(all_targets, all_preds)
        roc_auc = roc_auc_score(all_targets, all_outputs)
        precision_vals, recall_vals, _ = precision_recall_curve(all_targets, all_outputs)
        pr_auc = auc(recall_vals, precision_vals)        
        
        wandb.log({
            "val_loss": epoch_loss.avg,
            "val_accuracy": accuracy,
            "val_precision": precision,
            "val_recall": recall,
            "val_f1_score": F1,
            "val_roc_auc": roc_auc,
            "val_pr_auc": pr_auc
        })

        print(f"Accuracy: {accuracy}, Loss: {epoch_loss.avg}, Precision: {precision}, Recall: {recall}, F1: {F1}, roc_auc: {roc_auc}, pr_auc: {pr_auc}")

        logger.info('--- Val: \tLoss: {:.6f}  \tAccuracy: {:.3f} F1: {:.3f} Precision: {:.3f} Recall: {:.3f} Roc AUC: {:.3f} PR AUC {:.3f} --- RAM: {:.2f}/{:.2f} GB  gpu_mem: {:.2f} GB '.format(
                epoch_loss.avg, accuracy, F1, precision, recall, roc_auc, pr_auc, *misc.cpu_mem_usage(), misc.gpu_mem_usage()))
    else:
        precision = precision_score(all_targets, all_preds, average='macro')
        recall = recall_score(all_targets, all_preds, average='macro')
        F1 = f1_score(all_targets, all_preds, average='macro')
        accuracy = accuracy_score(all_targets, all_preds)
        try:
            roc_auc = roc_auc_score(all_targets, all_outputs, multi_class='ovr')  # One-vs-Rest strategy
        except:
            roc_auc = 0

        # Precision-recall curve 
        precision_vals = dict()
        recall_vals = dict()
        pr_auc_vals = dict()

        for class_idx in range(args.n_classes):
            precision_vals[class_idx], recall_vals[class_idx], _ = precision_recall_curve(
                (all_targets == class_idx).astype(int), all_outputs[:, class_idx]
            )
            pr_auc_vals[class_idx] = auc(recall_vals[class_idx], precision_vals[class_idx])

        # Compute average PR-AUC
        pr_auc = np.mean(list(pr_auc_vals.values()))

        wandb.log({
            "val_loss": epoch_loss.avg,
            "val_accuracy": accuracy,
            "val_precision_macro": precision,
            "val_recall_macro": recall,
            "val_f1_score_macro": F1,
            "val_roc_auc": roc_auc,
            "val_pr_auc": pr_auc
        })
        
        print(f"Accuracy: {accuracy}, Loss: {epoch_loss.avg}, Precision (macro): {precision}, Recall (macro): {recall}, F1 (macro): {F1}, roc_auc: {roc_auc}, pr_auc: {pr_auc}")

        logger.info('--- Val: \tLoss: {:.6f}  \tAccuracy: {:.3f} F1: {:.3f} Precision: {:.3f} Recall: {:.3f} Roc AUC: {:.3f} PR AUC {:.3f} --- RAM: {:.2f}/{:.2f} GB  gpu_mem: {:.2f} GB '.format(
                epoch_loss.avg, accuracy, F1, precision, recall, roc_auc, pr_auc, *misc.cpu_mem_usage(), misc.gpu_mem_usage()))
    
    return epoch_loss.avg, accuracy, precision, recall, F1, roc_auc, pr_auc

if __name__ == '__main__':
    main()
